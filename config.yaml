
paths:
  checkpoint_dir: checkpoints
  data_dir: datasets # directory to store processed data

preprocessing:
  languages: ['de', 'en_us']
  text_symbols: 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZäöüÄÖÜß'
  phoneme_symbols: ['a', 'b', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', 'ç', 'ð', 'ø', 'ŋ', 'œ', 'ɐ', 'ɑ', 'ɔ', 'ə', 'ɛ', 'ɝ', 'ɡ', 'ɪ', 'ʁ', 'ʃ', 'ʊ', 'ʌ', 'ʏ', 'ʒ', 'ʔ', 'ˈ', 'ˌ', 'ː', '̃', '̍', '̥', '̩', '̯', '͡', 'θ']
  tokenizer_start_index: 1
  tokenizer_end_index: 2
  lowercase: false
  n_val: 500

model:
  lstm_dim: 256
  conv_dim: 256

training:
  learning_rate: 0.0001
  batch_size: 32
  epochs: 100
  generate_steps: 10
  validate_steps: 1000
  checkpoint_steps: 100000
  n_generate_samples: 10
