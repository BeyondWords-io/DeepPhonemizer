
paths:
  checkpoint_dir: checkpoints
  data_dir: datasets # directory to store processed data

preprocessing:
  languages: ['de']
  text_symbols: 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZäöüÄÖÜß'
  phoneme_symbols: ['a', 'b', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', 'ç', 'ð', 'ø', 'ŋ', 'œ', 'ɐ', 'ɑ', 'ɔ', 'ə', 'ɛ', 'ɝ', 'ɡ', 'ɪ', 'ʁ', 'ʃ', 'ʊ', 'ʌ', 'ʏ', 'ʒ', 'ʔ', 'ˈ', 'ˌ', 'ː', '̃', '̍', '̥', '̩', '̯', '͡', 'θ']
  char_repeats: 3 # repeating chars of input text to enable mapping to longer phoneme sequences
  lowercase: true
  n_val: 100

model:
  type: 'transformer'  # choices: ['transformer', 'lstm']

  # transformer params
  d_model: 512
  d_fft: 1024
  layers: 1
  dropout: 0.1
  heads: 4

  # lstm params
  lstm_dim: 256
  num_layers: 2

training:
  learning_rate: 0.0001
  batch_size: 32
  batch_size_val: 32
  epochs: 100
  generate_steps: 10000
  validate_steps: 1000
  checkpoint_steps: 100000
  n_generate_samples: 10
